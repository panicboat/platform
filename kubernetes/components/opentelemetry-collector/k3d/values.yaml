# OpenTelemetry Collector Configuration for k3d
# Production-ready base configuration with k3d-specific overrides

# =============================================================================
# Deployment Mode
# =============================================================================
mode: deployment

# =============================================================================
# Service Configuration
# =============================================================================
service:
  # ClusterIP is sufficient - Beyla connects via hostPort (localhost:4317)
  type: ClusterIP

# =============================================================================
# Image Configuration
# =============================================================================
image:
  repository: otel/opentelemetry-collector-contrib
  tag: 0.143.1

# =============================================================================
# Monitoring
# =============================================================================
serviceMonitor:
  enabled: true
prometheusRule:
  enabled: false

# =============================================================================
# Collector Configuration
# =============================================================================
config:
  # ---------------------------------------------------------------------------
  # Receivers
  # ---------------------------------------------------------------------------
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

  # ---------------------------------------------------------------------------
  # Processors
  # ---------------------------------------------------------------------------
  processors:
    batch:
      send_batch_size: 1000
      send_batch_max_size: 1500
      timeout: 10s
    resource:
      attributes:
        - key: cluster.name
          # TODO(k3d): Change to actual cluster name in production
          value: k8s-local
          action: upsert
    memory_limiter:
      check_interval: 1s
      limit_mib: 400
      spike_limit_mib: 100
    # Transform processor to extract Kubernetes metadata from log body
    transform/logs:
      error_mode: ignore
      log_statements:
        - context: log
          statements:
            - set(resource.attributes["k8s.namespace.name"], body["kubernetes"]["namespace_name"]) where body["kubernetes"]["namespace_name"] != nil
            - set(resource.attributes["k8s.pod.name"], body["kubernetes"]["pod_name"]) where body["kubernetes"]["pod_name"] != nil
            - set(resource.attributes["k8s.container.name"], body["kubernetes"]["container_name"]) where body["kubernetes"]["container_name"] != nil
            - set(resource.attributes["service.name"], body["kubernetes"]["pod_name"]) where body["kubernetes"]["pod_name"] != nil and resource.attributes["service.name"] == nil
            - set(body, body["log"]) where body["log"] != nil

  # ---------------------------------------------------------------------------
  # Exporters
  # ---------------------------------------------------------------------------
  exporters:
    # Prometheus exporter for metrics
    prometheus:
      endpoint: "0.0.0.0:8889"
      send_timestamps: true
      metric_expiration: 180m
      enable_open_metrics: true
    # Debug exporter for troubleshooting
    debug:
      verbosity: basic
      sampling_initial: 5
      sampling_thereafter: 200
    # Loki exporter for logs
    otlphttp/loki:
      endpoint: "http://loki.monitoring.svc.cluster.local:3100/otlp"
      tls:
        insecure: true
    # Tempo exporter for traces
    otlp/tempo:
      endpoint: "tempo.monitoring.svc.cluster.local:4317"
      tls:
        insecure: true
      sending_queue:
        enabled: true
        num_consumers: 10
        queue_size: 5000
      retry_on_failure:
        enabled: true
        initial_interval: 5s
        max_interval: 30s
        max_elapsed_time: 300s

  # ---------------------------------------------------------------------------
  # Extensions
  # ---------------------------------------------------------------------------
  extensions:
    health_check:
      endpoint: "0.0.0.0:13133"
    pprof:
      endpoint: 0.0.0.0:1777
    zpages:
      endpoint: 0.0.0.0:55679

  # ---------------------------------------------------------------------------
  # Service Pipelines
  # ---------------------------------------------------------------------------
  service:
    extensions: [health_check, pprof, zpages]
    pipelines:
      # Metrics pipeline
      metrics:
        receivers: [otlp]
        processors: [memory_limiter, resource, batch]
        exporters: [prometheus]
      # Traces pipeline
      traces:
        receivers: [otlp]
        processors: [memory_limiter, resource, batch]
        exporters: [otlp/tempo, debug]
      # Logs pipeline
      logs:
        receivers: [otlp]
        processors: [memory_limiter, transform/logs, batch]
        exporters: [otlphttp/loki]

# =============================================================================
# Port Configuration
# =============================================================================
ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    protocol: TCP
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    protocol: TCP
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    protocol: TCP
  prom-metrics:
    enabled: true
    containerPort: 8889
    servicePort: 8889
    protocol: TCP

# =============================================================================
# Resource Limits
# =============================================================================
resources:
  limits:
    cpu: 256m
    memory: 512Mi
  requests:
    cpu: 128m
    memory: 256Mi
