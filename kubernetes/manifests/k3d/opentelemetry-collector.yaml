---
# Source: opentelemetry-collector/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opentelemetry-collector
  namespace: opentelemetry-system
  labels:
    helm.sh/chart: opentelemetry-collector-0.143.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-collector
    app.kubernetes.io/version: "0.143.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
---
# Source: opentelemetry-collector/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opentelemetry-collector
  namespace: opentelemetry-system
  labels:
    helm.sh/chart: opentelemetry-collector-0.143.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-collector
    app.kubernetes.io/version: "0.143.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
data:
  relay: |
    exporters:
      debug:
        sampling_initial: 2
        sampling_thereafter: 500
        verbosity: detailed
      otlp/tempo:
        endpoint: tempo.monitoring.svc.cluster.local:4317
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_elapsed_time: 300s
          max_interval: 30s
        sending_queue:
          enabled: true
          num_consumers: 10
          queue_size: 5000
        tls:
          insecure: true
      otlphttp/loki:
        endpoint: http://loki.monitoring.svc.cluster.local:3100/otlp
        tls:
          insecure: true
      prometheus:
        enable_open_metrics: true
        endpoint: 0.0.0.0:8889
        metric_expiration: 180m
        send_timestamps: true
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      pprof:
        endpoint: 0.0.0.0:1777
      zpages:
        endpoint: 0.0.0.0:55679
    processors:
      batch:
        send_batch_max_size: 1500
        send_batch_size: 1000
        timeout: 10s
      k8sattributes:
        auth_type: serviceAccount
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/name
            tag_name: app
          - from: pod
            key: app.kubernetes.io/version
            tag_name: version
          metadata:
          - k8s.namespace.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.node.name
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection
      memory_limiter:
        check_interval: 1s
        limit_mib: 400
        limit_percentage: 80
        spike_limit_mib: 100
        spike_limit_percentage: 25
      resource:
        attributes:
        - action: upsert
          key: cluster.name
          value: k8s-local
      transform/logs:
        error_mode: ignore
        log_statements:
        - context: log
          statements:
          - set(resource.attributes["k8s.pod.ip"], attributes["kubernetes"]["pod_ip"])
            where attributes["kubernetes"]["pod_ip"] != nil and resource.attributes["k8s.pod.ip"]
            == nil
          - set(resource.attributes["k8s.pod.uid"], attributes["kubernetes"]["pod_id"])
            where attributes["kubernetes"]["pod_id"] != nil and resource.attributes["k8s.pod.uid"]
            == nil
          - set(resource.attributes["k8s.namespace.name"], attributes["kubernetes"]["namespace_name"])
            where attributes["kubernetes"]["namespace_name"] != nil and resource.attributes["k8s.namespace.name"]
            == nil
          - set(resource.attributes["k8s.pod.name"], attributes["kubernetes"]["pod_name"])
            where attributes["kubernetes"]["pod_name"] != nil and resource.attributes["k8s.pod.name"]
            == nil
          - set(resource.attributes["k8s.container.name"], attributes["kubernetes"]["container_name"])
            where attributes["kubernetes"]["container_name"] != nil and resource.attributes["k8s.container.name"]
            == nil
          - set(resource.attributes["service.name"], attributes["kubernetes"]["pod_name"])
            where attributes["kubernetes"]["pod_name"] != nil and resource.attributes["service.name"]
            == nil
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:14250
          thrift_compact:
            endpoint: ${env:MY_POD_IP}:6831
          thrift_http:
            endpoint: ${env:MY_POD_IP}:14268
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
            - targets:
              - ${env:MY_POD_IP}:8888
      zipkin:
        endpoint: ${env:MY_POD_IP}:9411
    service:
      extensions:
      - health_check
      - pprof
      - zpages
      pipelines:
        logs:
          exporters:
          - otlphttp/loki
          processors:
          - memory_limiter
          - transform/logs
          - k8sattributes
          - resource
          - batch
          receivers:
          - otlp
        metrics:
          exporters:
          - prometheus
          processors:
          - memory_limiter
          - k8sattributes
          - resource
          - batch
          receivers:
          - otlp
        traces:
          exporters:
          - otlp/tempo
          processors:
          - memory_limiter
          - k8sattributes
          - resource
          - batch
          receivers:
          - otlp
      telemetry:
        metrics:
          readers:
          - pull:
              exporter:
                prometheus:
                  host: ${env:MY_POD_IP}
                  port: 8888
---
# Source: opentelemetry-collector/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: opentelemetry-collector
  labels:
    helm.sh/chart: opentelemetry-collector-0.143.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-collector
    app.kubernetes.io/version: "0.143.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
rules:
  - apiGroups:
    - ""
    resources:
    - pods
    - namespaces
    - nodes
    verbs:
    - get
    - watch
    - list
  - apiGroups:
    - apps
    resources:
    - deployments
    - replicasets
    - daemonsets
    - statefulsets
    verbs:
    - get
    - watch
    - list
---
# Source: opentelemetry-collector/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: opentelemetry-collector
  labels:
    helm.sh/chart: opentelemetry-collector-0.143.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-collector
    app.kubernetes.io/version: "0.143.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: opentelemetry-collector
subjects:
- kind: ServiceAccount
  name: opentelemetry-collector
  namespace: opentelemetry-system
---
# Source: opentelemetry-collector/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-collector
  namespace: opentelemetry-system
  labels:
    helm.sh/chart: opentelemetry-collector-0.143.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-collector
    app.kubernetes.io/version: "0.143.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
    component: standalone-collector
spec:
  type: ClusterIP
  ports:
    
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: UDP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-thrift
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: metrics
      port: 8888
      targetPort: 8888
      protocol: TCP
    - name: otlp
      port: 4317
      targetPort: 4317
      protocol: TCP
      appProtocol: grpc
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: prom-metrics
      port: 8889
      targetPort: 8889
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
  selector:
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-collector
    component: standalone-collector
  internalTrafficPolicy: Cluster
---
# Source: opentelemetry-collector/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-collector
  namespace: opentelemetry-system
  labels:
    helm.sh/chart: opentelemetry-collector-0.143.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-collector
    app.kubernetes.io/version: "0.143.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetry-collector
      app.kubernetes.io/instance: opentelemetry-collector
      component: standalone-collector
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: d6328038b4641680872735935b97941a22362ad759544a15426458a78487b1a9
        
      labels:
        app.kubernetes.io/name: opentelemetry-collector
        app.kubernetes.io/instance: opentelemetry-collector
        component: standalone-collector
        
    spec:
      
      serviceAccountName: opentelemetry-collector
      automountServiceAccountToken: true
      securityContext:
        {}
      containers:
        - name: opentelemetry-collector
          args:
            - --config=/conf/relay.yaml
          securityContext:
            {}
          image: "otel/opentelemetry-collector-contrib:0.143.1"
          imagePullPolicy: IfNotPresent
          ports:
            
            - name: jaeger-compact
              containerPort: 6831
              protocol: UDP
            - name: jaeger-grpc
              containerPort: 14250
              protocol: TCP
            - name: jaeger-thrift
              containerPort: 14268
              protocol: TCP
            - name: metrics
              containerPort: 8888
              protocol: TCP
            - name: otlp
              containerPort: 4317
              protocol: TCP
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
            - name: prom-metrics
              containerPort: 8889
              protocol: TCP
            - name: zipkin
              containerPort: 9411
              protocol: TCP
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: GOMEMLIMIT
              value: "409MiB"
          livenessProbe:
            httpGet:
              path: /
              port: 13133
          readinessProbe:
            httpGet:
              path: /
              port: 13133
          resources:
            limits:
              cpu: 256m
              memory: 512Mi
            requests:
              cpu: 128m
              memory: 256Mi
          volumeMounts:
            - mountPath: /conf
              name: opentelemetry-collector-configmap
      volumes:
        - name: opentelemetry-collector-configmap
          configMap:
            name: opentelemetry-collector
            items:
              - key: relay
                path: relay.yaml
      hostNetwork: false
---
# Source: opentelemetry-collector/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: opentelemetry-collector
  namespace: opentelemetry-system
  labels:
    helm.sh/chart: opentelemetry-collector-0.143.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-collector
    app.kubernetes.io/version: "0.143.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetry-collector
      app.kubernetes.io/instance: opentelemetry-collector
      component: standalone-collector
  endpoints:
    - port: metrics
